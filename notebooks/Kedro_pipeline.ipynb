{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-14 08:42:52,179 - kedro.io.data_catalog - INFO - Loading data from `news` (CSVDataSet)...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>news</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I was wondering if anyone out there could enli...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>A fair number of brave souls who upgraded thei...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>well folks, my mac plus finally gave up the gh...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\\nDo you have Weitek's address/phone number?  ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>From article &lt;C5owCB.n3p@world.std.com&gt;, by to...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               news  labels\n",
       "0           0  I was wondering if anyone out there could enli...       7\n",
       "1           1  A fair number of brave souls who upgraded thei...       4\n",
       "2           2  well folks, my mac plus finally gave up the gh...       4\n",
       "3           3  \\nDo you have Weitek's address/phone number?  ...       1\n",
       "4           4  From article <C5owCB.n3p@world.std.com>, by to...      14"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalog.load('news').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'de': Pipeline([\n",
       " Node(split_data, ['steam_reviews', 'params:target', 'params:columns'], {'train_x': 'train_x', 'train_y': 'train_y', 'test_x': 'test_x', 'test_y': 'test_y'}, None)\n",
       " ]),\n",
       " 'prep': Pipeline([\n",
       " Node(get_DF, ['train_x', 'params:preprocess_out'], 'df_sents', None),\n",
       " Node(get_DF, ['test_x', 'params:preprocess_out'], 'df_sents_test', None)\n",
       " ]),\n",
       " 'ds': Pipeline([\n",
       " Node(train_model, ['train_x', 'train_y', 'parameters'], 'example_model', None),\n",
       " Node(predict, {'model': 'example_model', 'test_x': 'example_test_x'}, 'example_predictions', None),\n",
       " Node(report_accuracy, ['example_predictions', 'example_test_y'], None, None)\n",
       " ]),\n",
       " 'vec': Pipeline([\n",
       " Node(train_Corex, ['params:n_topics', 'df_sents'], {'corex_model': 'corex_model', 'countvectorizer': 'countvectorizer'}, None),\n",
       " Node(get_precBert, ['df_sents'], 'distill_vec', None),\n",
       " Node(get_precBert, ['df_sents_test'], 'distill_vec_t', None),\n",
       " Node(predict_Corex, ['df_sents', 'corex_model', 'countvectorizer'], {'corex_vecs_1': 'corex_vecs_1', 'corex_vecs_2': 'corex_vecs_2', 'textID': 'textID'}, None),\n",
       " Node(predict_Corex, ['df_sents_test', 'corex_model', 'countvectorizer'], {'corex_vecs_1': 'corex_vecs_1_t', 'corex_vecs_2': 'corex_vecs_2_t', 'textID': 'textID_t'}, None),\n",
       " Node(autoencoder, ['corex_vecs_2', 'distill_vec'], {'ae': 'auto_encoder_model'}, None),\n",
       " Node(ae_predict, ['corex_vecs_2', 'distill_vec', 'textID', 'auto_encoder_model'], {'vec': 'corex_encoded', 'uniqueID': 'textIDA'}, None),\n",
       " Node(ae_predict, ['corex_vecs_2_t', 'distill_vec_t', 'textID_t', 'auto_encoder_model'], {'vec': 'corex_encoded_t', 'uniqueID': 'textIDA_t'}, None),\n",
       " Node(get_mean_pooling, ['corex_encoded_t', 'textIDA_t'], 'corex_pooled_t', None),\n",
       " Node(get_mean_pooling, ['corex_encoded', 'textIDA'], 'corex_pooled', None)\n",
       " ]),\n",
       " '__default__': Pipeline([\n",
       " Node(split_data, ['steam_reviews', 'params:target', 'params:columns'], {'train_x': 'train_x', 'train_y': 'train_y', 'test_x': 'test_x', 'test_y': 'test_y'}, None),\n",
       " Node(get_DF, ['train_x', 'params:preprocess_out'], 'df_sents', None),\n",
       " Node(get_DF, ['test_x', 'params:preprocess_out'], 'df_sents_test', None),\n",
       " Node(train_Corex, ['params:n_topics', 'df_sents'], {'corex_model': 'corex_model', 'countvectorizer': 'countvectorizer'}, None),\n",
       " Node(get_precBert, ['df_sents'], 'distill_vec', None),\n",
       " Node(get_precBert, ['df_sents_test'], 'distill_vec_t', None),\n",
       " Node(predict_Corex, ['df_sents', 'corex_model', 'countvectorizer'], {'corex_vecs_1': 'corex_vecs_1', 'corex_vecs_2': 'corex_vecs_2', 'textID': 'textID'}, None),\n",
       " Node(predict_Corex, ['df_sents_test', 'corex_model', 'countvectorizer'], {'corex_vecs_1': 'corex_vecs_1_t', 'corex_vecs_2': 'corex_vecs_2_t', 'textID': 'textID_t'}, None),\n",
       " Node(autoencoder, ['corex_vecs_2', 'distill_vec'], {'ae': 'auto_encoder_model'}, None),\n",
       " Node(ae_predict, ['corex_vecs_2', 'distill_vec', 'textID', 'auto_encoder_model'], {'vec': 'corex_encoded', 'uniqueID': 'textIDA'}, None),\n",
       " ...\n",
       " ])}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context.pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-19 20:33:29,556 - root - INFO - ** Kedro project nlpipe\n",
      "2020-06-19 20:33:29,601 - kedro.io.data_catalog - INFO - Loading data from `steam_reviews` (CSVDataSet)...\n",
      "2020-06-19 20:33:31,140 - kedro.io.data_catalog - INFO - Loading data from `params:target` (MemoryDataSet)...\n",
      "2020-06-19 20:33:31,147 - kedro.io.data_catalog - INFO - Loading data from `params:columns` (MemoryDataSet)...\n",
      "2020-06-19 20:33:31,148 - kedro.pipeline.node - INFO - Running node: split_data([params:columns,params:target,steam_reviews]) -> [test_x,test_y,train_x,train_y]\n",
      "2020-06-19 20:33:31,255 - kedro.io.data_catalog - INFO - Saving data to `train_x` (MemoryDataSet)...\n",
      "2020-06-19 20:33:31,267 - kedro.io.data_catalog - INFO - Saving data to `train_y` (MemoryDataSet)...\n",
      "2020-06-19 20:33:31,270 - kedro.io.data_catalog - INFO - Saving data to `test_x` (MemoryDataSet)...\n",
      "2020-06-19 20:33:31,275 - kedro.io.data_catalog - INFO - Saving data to `test_y` (MemoryDataSet)...\n",
      "2020-06-19 20:33:31,309 - kedro.runner.sequential_runner - INFO - Completed 1 out of 19 tasks\n",
      "2020-06-19 20:33:31,311 - kedro.io.data_catalog - INFO - Loading data from `train_x` (MemoryDataSet)...\n",
      "2020-06-19 20:33:31,321 - kedro.io.data_catalog - INFO - Loading data from `params:preprocess_out` (MemoryDataSet)...\n",
      "2020-06-19 20:33:31,322 - kedro.pipeline.node - INFO - Running node: get_DF([params:preprocess_out,train_x]) -> [df_sents]\n",
      "2020-06-19 20:34:04,639 - kedro.io.data_catalog - INFO - Saving data to `df_sents` (MemoryDataSet)...\n",
      "2020-06-19 20:34:04,642 - kedro.runner.sequential_runner - INFO - Completed 2 out of 19 tasks\n",
      "2020-06-19 20:34:04,643 - kedro.io.data_catalog - INFO - Loading data from `test_x` (MemoryDataSet)...\n",
      "2020-06-19 20:34:04,647 - kedro.io.data_catalog - INFO - Loading data from `params:preprocess_out` (MemoryDataSet)...\n",
      "2020-06-19 20:34:04,648 - kedro.pipeline.node - INFO - Running node: get_DF([params:preprocess_out,test_x]) -> [df_sents_test]\n",
      "2020-06-19 20:34:13,673 - kedro.io.data_catalog - INFO - Saving data to `df_sents_test` (MemoryDataSet)...\n",
      "2020-06-19 20:34:13,676 - kedro.runner.sequential_runner - INFO - Completed 3 out of 19 tasks\n",
      "2020-06-19 20:34:13,677 - kedro.io.data_catalog - INFO - Loading data from `params:n_topics` (MemoryDataSet)...\n",
      "2020-06-19 20:34:13,678 - kedro.io.data_catalog - INFO - Loading data from `df_sents` (MemoryDataSet)...\n",
      "2020-06-19 20:34:13,680 - kedro.pipeline.node - INFO - Running node: train_Corex([df_sents,params:n_topics]) -> [corex_model,countvectorizer]\n",
      "13084\n",
      "WARNING: Some words never appear (or always appear)\n",
      "2020-06-19 20:34:53,347 - kedro.io.data_catalog - INFO - Saving data to `corex_model` (MemoryDataSet)...\n",
      "2020-06-19 20:34:53,391 - kedro.io.data_catalog - INFO - Saving data to `countvectorizer` (MemoryDataSet)...\n",
      "2020-06-19 20:34:53,434 - kedro.runner.sequential_runner - INFO - Completed 4 out of 19 tasks\n",
      "2020-06-19 20:34:53,435 - kedro.io.data_catalog - INFO - Loading data from `df_sents` (MemoryDataSet)...\n",
      "2020-06-19 20:34:53,437 - kedro.pipeline.node - INFO - Running node: get_precBert([df_sents]) -> [distill_vec]\n",
      "2020-06-19 20:34:53,438 - root - INFO - Load pretrained SentenceTransformer: bert-base-nli-stsb-mean-tokens\n",
      "2020-06-19 20:34:53,438 - root - INFO - Did not find a '/' or '\\' in the name. Assume to download model from server.\n",
      "2020-06-19 20:34:53,445 - root - INFO - Load SentenceTransformer from folder: /Users/karthik/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-stsb-mean-tokens.zip\n",
      "2020-06-19 20:34:53,466 - transformers.configuration_utils - INFO - loading configuration file /Users/karthik/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-stsb-mean-tokens.zip/0_BERT/config.json\n",
      "2020-06-19 20:34:53,467 - transformers.configuration_utils - INFO - Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "2020-06-19 20:34:53,468 - transformers.modeling_utils - INFO - loading weights file /Users/karthik/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-stsb-mean-tokens.zip/0_BERT/pytorch_model.bin\n",
      "2020-06-19 20:34:55,070 - transformers.tokenization_utils - INFO - Model name '/Users/karthik/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-stsb-mean-tokens.zip/0_BERT' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '/Users/karthik/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-stsb-mean-tokens.zip/0_BERT' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "2020-06-19 20:34:55,071 - transformers.tokenization_utils - INFO - Didn't find file /Users/karthik/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-stsb-mean-tokens.zip/0_BERT/tokenizer_config.json. We won't load it.\n",
      "2020-06-19 20:34:55,072 - transformers.tokenization_utils - INFO - loading file /Users/karthik/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-stsb-mean-tokens.zip/0_BERT/vocab.txt\n",
      "2020-06-19 20:34:55,073 - transformers.tokenization_utils - INFO - loading file /Users/karthik/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-stsb-mean-tokens.zip/0_BERT/added_tokens.json\n",
      "2020-06-19 20:34:55,074 - transformers.tokenization_utils - INFO - loading file /Users/karthik/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-stsb-mean-tokens.zip/0_BERT/special_tokens_map.json\n",
      "2020-06-19 20:34:55,074 - transformers.tokenization_utils - INFO - loading file None\n",
      "2020-06-19 20:34:55,105 - root - INFO - Use pytorch device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1636/1636 [04:23<00:00,  6.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-19 20:39:18,484 - kedro.io.data_catalog - INFO - Saving data to `distill_vec` (MemoryDataSet)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-19 20:39:18,521 - kedro.runner.sequential_runner - INFO - Completed 5 out of 19 tasks\n",
      "2020-06-19 20:39:18,523 - kedro.io.data_catalog - INFO - Loading data from `df_sents_test` (MemoryDataSet)...\n",
      "2020-06-19 20:39:18,526 - kedro.pipeline.node - INFO - Running node: get_precBert([df_sents_test]) -> [distill_vec_t]\n",
      "2020-06-19 20:39:18,534 - root - INFO - Load pretrained SentenceTransformer: bert-base-nli-stsb-mean-tokens\n",
      "2020-06-19 20:39:18,536 - root - INFO - Did not find a '/' or '\\' in the name. Assume to download model from server.\n",
      "2020-06-19 20:39:18,538 - root - INFO - Load SentenceTransformer from folder: /Users/karthik/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-stsb-mean-tokens.zip\n",
      "2020-06-19 20:39:18,546 - transformers.configuration_utils - INFO - loading configuration file /Users/karthik/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-stsb-mean-tokens.zip/0_BERT/config.json\n",
      "2020-06-19 20:39:18,549 - transformers.configuration_utils - INFO - Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "2020-06-19 20:39:18,550 - transformers.modeling_utils - INFO - loading weights file /Users/karthik/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-stsb-mean-tokens.zip/0_BERT/pytorch_model.bin\n",
      "2020-06-19 20:39:20,600 - transformers.tokenization_utils - INFO - Model name '/Users/karthik/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-stsb-mean-tokens.zip/0_BERT' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '/Users/karthik/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-stsb-mean-tokens.zip/0_BERT' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "2020-06-19 20:39:20,602 - transformers.tokenization_utils - INFO - Didn't find file /Users/karthik/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-stsb-mean-tokens.zip/0_BERT/tokenizer_config.json. We won't load it.\n",
      "2020-06-19 20:39:20,603 - transformers.tokenization_utils - INFO - loading file /Users/karthik/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-stsb-mean-tokens.zip/0_BERT/vocab.txt\n",
      "2020-06-19 20:39:20,604 - transformers.tokenization_utils - INFO - loading file /Users/karthik/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-stsb-mean-tokens.zip/0_BERT/added_tokens.json\n",
      "2020-06-19 20:39:20,604 - transformers.tokenization_utils - INFO - loading file /Users/karthik/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-stsb-mean-tokens.zip/0_BERT/special_tokens_map.json\n",
      "2020-06-19 20:39:20,605 - transformers.tokenization_utils - INFO - loading file None\n",
      "2020-06-19 20:39:20,720 - root - INFO - Use pytorch device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 466/466 [01:25<00:00,  5.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-19 20:40:46,064 - kedro.io.data_catalog - INFO - Saving data to `distill_vec_t` (MemoryDataSet)...\n",
      "2020-06-19 20:40:46,091 - kedro.runner.sequential_runner - INFO - Completed 6 out of 19 tasks\n",
      "2020-06-19 20:40:46,094 - kedro.io.data_catalog - INFO - Loading data from `df_sents` (MemoryDataSet)...\n",
      "2020-06-19 20:40:46,098 - kedro.io.data_catalog - INFO - Loading data from `corex_model` (MemoryDataSet)...\n",
      "2020-06-19 20:40:46,165 - kedro.io.data_catalog - INFO - Loading data from `countvectorizer` (MemoryDataSet)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-19 20:40:46,223 - kedro.pipeline.node - INFO - Running node: predict_Corex([corex_model,countvectorizer,df_sents]) -> [corex_vecs_1,corex_vecs_2,textID]\n",
      "2020-06-19 20:40:46,482 - kedro.io.data_catalog - INFO - Saving data to `corex_vecs_1` (MemoryDataSet)...\n",
      "2020-06-19 20:40:46,483 - kedro.io.data_catalog - INFO - Saving data to `corex_vecs_2` (MemoryDataSet)...\n",
      "2020-06-19 20:40:46,485 - kedro.io.data_catalog - INFO - Saving data to `textID` (MemoryDataSet)...\n",
      "2020-06-19 20:40:46,499 - kedro.runner.sequential_runner - INFO - Completed 7 out of 19 tasks\n",
      "2020-06-19 20:40:46,501 - kedro.io.data_catalog - INFO - Loading data from `df_sents_test` (MemoryDataSet)...\n",
      "2020-06-19 20:40:46,503 - kedro.io.data_catalog - INFO - Loading data from `corex_model` (MemoryDataSet)...\n",
      "2020-06-19 20:40:46,615 - kedro.io.data_catalog - INFO - Loading data from `countvectorizer` (MemoryDataSet)...\n",
      "2020-06-19 20:40:46,736 - kedro.pipeline.node - INFO - Running node: predict_Corex([corex_model,countvectorizer,df_sents_test]) -> [corex_vecs_1_t,corex_vecs_2_t,textID_t]\n",
      "2020-06-19 20:40:46,847 - kedro.io.data_catalog - INFO - Saving data to `corex_vecs_1_t` (MemoryDataSet)...\n",
      "2020-06-19 20:40:46,848 - kedro.io.data_catalog - INFO - Saving data to `corex_vecs_2_t` (MemoryDataSet)...\n",
      "2020-06-19 20:40:46,849 - kedro.io.data_catalog - INFO - Saving data to `textID_t` (MemoryDataSet)...\n",
      "2020-06-19 20:40:46,857 - kedro.runner.sequential_runner - INFO - Completed 8 out of 19 tasks\n",
      "2020-06-19 20:40:46,858 - kedro.io.data_catalog - INFO - Loading data from `corex_vecs_2` (MemoryDataSet)...\n",
      "2020-06-19 20:40:46,859 - kedro.io.data_catalog - INFO - Loading data from `distill_vec` (MemoryDataSet)...\n",
      "2020-06-19 20:40:46,897 - kedro.pipeline.node - INFO - Running node: autoencoder([corex_vecs_2,distill_vec]) -> [auto_encoder_model]\n",
      "(13084, 798)\n",
      "(13084, 798)\n",
      "Fitting Autoencoder ...\n",
      "Fitting Autoencoder Done!\n",
      "2020-06-19 20:41:47,774 - kedro.io.data_catalog - INFO - Saving data to `auto_encoder_model` (MemoryDataSet)...\n",
      "2020-06-19 20:41:47,787 - kedro.runner.sequential_runner - INFO - Completed 9 out of 19 tasks\n",
      "2020-06-19 20:41:47,788 - kedro.io.data_catalog - INFO - Loading data from `corex_vecs_2` (MemoryDataSet)...\n",
      "2020-06-19 20:41:47,789 - kedro.io.data_catalog - INFO - Loading data from `distill_vec` (MemoryDataSet)...\n",
      "2020-06-19 20:41:47,797 - kedro.io.data_catalog - INFO - Loading data from `textID` (MemoryDataSet)...\n",
      "2020-06-19 20:41:47,806 - kedro.io.data_catalog - INFO - Loading data from `auto_encoder_model` (MemoryDataSet)...\n",
      "2020-06-19 20:41:47,815 - kedro.pipeline.node - INFO - Running node: ae_predict([auto_encoder_model,corex_vecs_2,distill_vec,textID]) -> [corex_encoded,textIDA]\n",
      "(13084, 798)\n",
      "2020-06-19 20:41:47,974 - kedro.io.data_catalog - INFO - Saving data to `corex_encoded` (MemoryDataSet)...\n",
      "2020-06-19 20:41:47,976 - kedro.io.data_catalog - INFO - Saving data to `textIDA` (MemoryDataSet)...\n",
      "2020-06-19 20:41:48,000 - kedro.runner.sequential_runner - INFO - Completed 10 out of 19 tasks\n",
      "2020-06-19 20:41:48,001 - kedro.io.data_catalog - INFO - Loading data from `corex_vecs_2_t` (MemoryDataSet)...\n",
      "2020-06-19 20:41:48,002 - kedro.io.data_catalog - INFO - Loading data from `distill_vec_t` (MemoryDataSet)...\n",
      "2020-06-19 20:41:48,009 - kedro.io.data_catalog - INFO - Loading data from `textID_t` (MemoryDataSet)...\n",
      "2020-06-19 20:41:48,013 - kedro.io.data_catalog - INFO - Loading data from `auto_encoder_model` (MemoryDataSet)...\n",
      "2020-06-19 20:41:48,021 - kedro.pipeline.node - INFO - Running node: ae_predict([auto_encoder_model,corex_vecs_2_t,distill_vec_t,textID_t]) -> [corex_encoded_t,textIDA_t]\n",
      "(3723, 798)\n",
      "2020-06-19 20:41:48,073 - kedro.io.data_catalog - INFO - Saving data to `corex_encoded_t` (MemoryDataSet)...\n",
      "2020-06-19 20:41:48,074 - kedro.io.data_catalog - INFO - Saving data to `textIDA_t` (MemoryDataSet)...\n",
      "2020-06-19 20:41:48,082 - kedro.runner.sequential_runner - INFO - Completed 11 out of 19 tasks\n",
      "2020-06-19 20:41:48,084 - kedro.io.data_catalog - INFO - Loading data from `corex_encoded_t` (MemoryDataSet)...\n",
      "2020-06-19 20:41:48,084 - kedro.io.data_catalog - INFO - Loading data from `textIDA_t` (MemoryDataSet)...\n",
      "2020-06-19 20:41:48,086 - kedro.pipeline.node - INFO - Running node: get_mean_pooling([corex_encoded_t,textIDA_t]) -> [corex_pooled_t]\n",
      "(3723, 2)\n",
      "(870, 64)\n",
      "2020-06-19 20:41:48,279 - kedro.io.data_catalog - INFO - Saving data to `corex_pooled_t` (MemoryDataSet)...\n",
      "2020-06-19 20:41:48,280 - kedro.runner.sequential_runner - INFO - Completed 12 out of 19 tasks\n",
      "2020-06-19 20:41:48,281 - kedro.io.data_catalog - INFO - Loading data from `corex_encoded` (MemoryDataSet)...\n",
      "2020-06-19 20:41:48,284 - kedro.io.data_catalog - INFO - Loading data from `textIDA` (MemoryDataSet)...\n",
      "2020-06-19 20:41:48,291 - kedro.pipeline.node - INFO - Running node: get_mean_pooling([corex_encoded,textIDA]) -> [corex_pooled]\n",
      "(13084, 2)\n",
      "(3479, 64)\n",
      "2020-06-19 20:41:49,009 - kedro.io.data_catalog - INFO - Saving data to `corex_pooled` (MemoryDataSet)...\n",
      "2020-06-19 20:41:49,013 - kedro.runner.sequential_runner - INFO - Completed 13 out of 19 tasks\n",
      "2020-06-19 20:41:49,014 - kedro.io.data_catalog - INFO - Loading data from `corex_pooled` (MemoryDataSet)...\n",
      "2020-06-19 20:41:49,015 - kedro.io.data_catalog - INFO - Loading data from `train_y` (MemoryDataSet)...\n",
      "2020-06-19 20:41:49,017 - kedro.pipeline.node - INFO - Running node: RF_train([corex_pooled,train_y]) -> [RF_model]\n",
      "2020-06-19 20:41:50,838 - kedro.io.data_catalog - INFO - Saving data to `RF_model` (MemoryDataSet)...\n",
      "2020-06-19 20:41:50,856 - kedro.runner.sequential_runner - INFO - Completed 14 out of 19 tasks\n",
      "2020-06-19 20:41:50,857 - kedro.io.data_catalog - INFO - Loading data from `corex_pooled` (MemoryDataSet)...\n",
      "2020-06-19 20:41:50,858 - kedro.io.data_catalog - INFO - Loading data from `params:n_topics` (MemoryDataSet)...\n",
      "2020-06-19 20:41:50,858 - kedro.pipeline.node - INFO - Running node: gmm_train([corex_pooled,params:n_topics]) -> [gmm]\n",
      "2020-06-19 20:41:53,788 - kedro.io.data_catalog - INFO - Saving data to `gmm` (MemoryDataSet)...\n",
      "2020-06-19 20:41:53,791 - kedro.runner.sequential_runner - INFO - Completed 15 out of 19 tasks\n",
      "2020-06-19 20:41:53,792 - kedro.io.data_catalog - INFO - Loading data from `gmm` (MemoryDataSet)...\n",
      "2020-06-19 20:41:53,794 - kedro.io.data_catalog - INFO - Loading data from `corex_pooled_t` (MemoryDataSet)...\n",
      "2020-06-19 20:41:53,795 - kedro.pipeline.node - INFO - Running node: model_predict([corex_pooled_t,gmm]) -> [cluster_out]\n",
      "2020-06-19 20:41:53,808 - kedro.io.data_catalog - INFO - Saving data to `cluster_out` (MemoryDataSet)...\n",
      "2020-06-19 20:41:53,810 - kedro.runner.sequential_runner - INFO - Completed 16 out of 19 tasks\n",
      "2020-06-19 20:41:53,811 - kedro.io.data_catalog - INFO - Loading data from `RF_model` (MemoryDataSet)...\n",
      "2020-06-19 20:41:53,826 - kedro.io.data_catalog - INFO - Loading data from `corex_pooled_t` (MemoryDataSet)...\n",
      "2020-06-19 20:41:53,827 - kedro.pipeline.node - INFO - Running node: model_predict([RF_model,corex_pooled_t]) -> [predictions]\n",
      "2020-06-19 20:41:53,832 - kedro.io.data_catalog - INFO - Saving data to `predictions` (MemoryDataSet)...\n",
      "2020-06-19 20:41:53,834 - kedro.runner.sequential_runner - INFO - Completed 17 out of 19 tasks\n",
      "2020-06-19 20:41:53,836 - kedro.io.data_catalog - INFO - Loading data from `test_y` (MemoryDataSet)...\n",
      "2020-06-19 20:41:53,838 - kedro.io.data_catalog - INFO - Loading data from `predictions` (MemoryDataSet)...\n",
      "2020-06-19 20:41:53,839 - kedro.pipeline.node - INFO - Running node: report_gen([predictions,test_y]) -> [report]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.62      0.66       268\n",
      "           1       0.84      0.89      0.86       602\n",
      "\n",
      "    accuracy                           0.81       870\n",
      "   macro avg       0.78      0.75      0.76       870\n",
      "weighted avg       0.80      0.81      0.80       870\n",
      "\n",
      "2020-06-19 20:41:53,854 - kedro.io.data_catalog - INFO - Saving data to `report` (MemoryDataSet)...\n",
      "2020-06-19 20:41:53,855 - kedro.runner.sequential_runner - INFO - Completed 18 out of 19 tasks\n",
      "2020-06-19 20:41:53,856 - kedro.io.data_catalog - INFO - Loading data from `corex_pooled_t` (MemoryDataSet)...\n",
      "2020-06-19 20:41:53,857 - kedro.io.data_catalog - INFO - Loading data from `test_y` (MemoryDataSet)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-19 20:41:53,859 - kedro.io.data_catalog - INFO - Loading data from `cluster_out` (MemoryDataSet)...\n",
      "2020-06-19 20:41:53,860 - kedro.pipeline.node - INFO - Running node: umap_iplot([cluster_out,corex_pooled_t,test_y]) -> [umap_vecs]\n",
      "2020-06-19 20:42:01,365 - kedro.io.data_catalog - INFO - Saving data to `umap_vecs` (MemoryDataSet)...\n",
      "2020-06-19 20:42:01,374 - kedro.runner.sequential_runner - INFO - Completed 19 out of 19 tasks\n",
      "2020-06-19 20:42:01,376 - kedro.runner.sequential_runner - INFO - Pipeline execution completed successfully.\n",
      "2020-06-19 20:42:01,377 - kedro.io.data_catalog - INFO - Loading data from `corex_vecs_1_t` (MemoryDataSet)...\n",
      "2020-06-19 20:42:01,379 - kedro.io.data_catalog - INFO - Loading data from `report` (MemoryDataSet)...\n",
      "2020-06-19 20:42:01,381 - kedro.io.data_catalog - INFO - Loading data from `corex_vecs_1` (MemoryDataSet)...\n",
      "2020-06-19 20:42:01,384 - kedro.io.data_catalog - INFO - Loading data from `umap_vecs` (MemoryDataSet)...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'corex_vecs_1_t': array([[6.10971093e-03, 1.09270210e-02, 1.12078229e-02, ...,\n",
       "         1.95158889e-01, 2.82182576e-01, 3.66823404e-01],\n",
       "        [6.10971093e-03, 1.09270210e-02, 1.12078229e-02, ...,\n",
       "         1.95213138e-01, 2.82327985e-01, 3.66968414e-01],\n",
       "        [9.99990804e-01, 8.51398476e-06, 1.12078229e-02, ...,\n",
       "         1.96935519e-01, 2.81916362e-01, 3.65285766e-01],\n",
       "        ...,\n",
       "        [6.10971093e-03, 1.09270210e-02, 1.12078229e-02, ...,\n",
       "         1.95182282e-01, 2.82418478e-01, 3.66941890e-01],\n",
       "        [6.10971093e-03, 1.09270210e-02, 1.12078229e-02, ...,\n",
       "         1.95182078e-01, 2.82434664e-01, 3.66952438e-01],\n",
       "        [1.82055972e-01, 1.09270210e-02, 1.12078229e-02, ...,\n",
       "         1.95137354e-01, 2.83978293e-01, 3.66886205e-01]]),\n",
       " 'report': '              precision    recall  f1-score   support\\n\\n           0       0.71      0.62      0.66       268\\n           1       0.84      0.89      0.86       602\\n\\n    accuracy                           0.81       870\\n   macro avg       0.78      0.75      0.76       870\\nweighted avg       0.80      0.81      0.80       870\\n',\n",
       " 'corex_vecs_1': array([[0.00610971, 0.01092702, 0.01120782, ..., 0.19516315, 0.28222557,\n",
       "         0.36665374],\n",
       "        [0.999999  , 0.99814088, 0.11218298, ..., 0.19518328, 0.28239452,\n",
       "         0.36695476],\n",
       "        [0.00610971, 0.999999  , 0.01120782, ..., 0.19517155, 0.28232686,\n",
       "         0.36665809],\n",
       "        ...,\n",
       "        [0.00610971, 0.01092702, 0.01120782, ..., 0.19519183, 0.28234815,\n",
       "         0.36683612],\n",
       "        [0.06586687, 0.01092702, 0.07057741, ..., 0.19514803, 0.28265433,\n",
       "         0.36688367],\n",
       "        [0.00610971, 0.01092702, 0.01120782, ..., 0.19511467, 0.28330082,\n",
       "         0.36686142]]),\n",
       " 'umap_vecs': UMAP(a=None, angular_rp_forest=False, b=None,\n",
       "      force_approximation_algorithm=False, init='spectral', learning_rate=1.0,\n",
       "      local_connectivity=1.0, low_memory=False, metric='euclidean',\n",
       "      metric_kwds=None, min_dist=0.1, n_components=2, n_epochs=None,\n",
       "      n_neighbors=15, negative_sample_rate=5, output_metric='euclidean',\n",
       "      output_metric_kwds=None, random_state=None, repulsion_strength=1.0,\n",
       "      set_op_mix_ratio=1.0, spread=1.0, target_metric='categorical',\n",
       "      target_metric_kwds=None, target_n_neighbors=-1, target_weight=0.5,\n",
       "      transform_queue_size=4.0, transform_seed=42, unique=False, verbose=False)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpipe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
